{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from typing import Dict, List, Counter\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.path.join('..', 'processed_data')\n",
    "DATA_PATH = os.path.join('..', 'data_2', 'Keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_use_file(filepath) -> bool:\n",
    "    # if file path is wrong for some reason, do not use it\n",
    "    if not os.path.exists(filepath):\n",
    "        return False\n",
    "\n",
    "    file_stats = os.stat(filepath)\n",
    "\n",
    "    # if file is empty, do not use it\n",
    "    if file_stats.st_size == 0:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths(data_path, folders_to_ignore = []):\n",
    "    result = []\n",
    "\n",
    "    # print(os.listdir(os.path.join('..')))\n",
    "    for folder_name in os.listdir(data_path):\n",
    "        if folder_name in folders_to_ignore:\n",
    "            continue\n",
    "\n",
    "        folder_path = os.path.join(data_path, folder_name)\n",
    "\n",
    "        if not os.path.isdir(folder_path): # it's a file\n",
    "            if folder_path.endswith('.ann') and should_use_file(folder_path): # only work with .ann files\n",
    "                result.append(os.path.splitext(folder_path)[0])\n",
    "\n",
    "            continue\n",
    "\n",
    "        sub_paths = get_filepaths(folder_path)\n",
    "        if len(sub_paths) > 0:\n",
    "            result.extend(sub_paths)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = get_filepaths(DATA_PATH, folders_to_ignore=['6847', 'Charles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2490"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_line(line_text):\n",
    "    '''\n",
    "        Validate if a line is not marked as 'transcription error' or as a duplicated one.\n",
    "        If any of those is true, this whole document must be skipped\n",
    "    '''\n",
    "    invalid = line_text.startswith('TranscriptionError') or line_text.startswith('DuplicatePage')\n",
    "    return not invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants():\n",
    "    Empty = 'O'\n",
    "    Beginning = 'B-'\n",
    "    Inside = 'I-'\n",
    "\n",
    "    MainEntityPrefix = 'T'\n",
    "    SubEntityPrefix = 'A'\n",
    "\n",
    "entities_to_cols = {\n",
    "    'main': 'NE-MAIN',\n",
    "    'gender': 'NE-PER-GENDER',\n",
    "    'legalstatus': 'NE-PER-LEGAL-STATUS',\n",
    "    'role': 'NE-PER-ROLE',\n",
    "    'misc': 'MISC',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotator_from_filepath(filepath:str):\n",
    "    normalized_path = os.path.normpath(filepath)\n",
    "    split_path = normalized_path.split(os.sep)\n",
    "    main_folder_index = split_path.index('Keep')\n",
    "    annotator = split_path[main_folder_index + 2]\n",
    "\n",
    "    return annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotationWriter:\n",
    "    def __init__(self):\n",
    "        self._fieldnames = ['TOKEN', 'NE-MAIN', 'NE-PER-GENDER', 'NE-PER-LEGAL-STATUS', 'NE-PER-ROLE', 'MISC']\n",
    "\n",
    "        self._train_file_handler, self._train_csv_writer = self._prepare_output_file('train')\n",
    "        self._dev_file_handler, self._dev_csv_writer = self._prepare_output_file('dev')\n",
    "        self._test_file_handler, self._test_csv_writer = self._prepare_output_file('test')\n",
    "\n",
    "        self._stats = {}\n",
    "\n",
    "    def write_annotation(self, filepath: str, annotations: Dict[int, Dict[str, str]]):\n",
    "        csv_writer, output_type = self._get_csv_writer()\n",
    "\n",
    "        csv_writer.writerow({})\n",
    "        csv_writer.writerow({'TOKEN': '# language = nl'})\n",
    "        csv_writer.writerow({'TOKEN': f'# document_path = {filepath}.txt'})\n",
    "        # annotator = get_annotator_from_filepath(filepath)\n",
    "        # csv_writer.writerow({'TOKEN': f'# annotator = {annotator}'})\n",
    "\n",
    "        char_counter = 1\n",
    "        with open(f'{filepath}.txt', 'r', encoding='utf-8') as file_handle:\n",
    "            file_content = file_handle.read().replace('\\n', ' ')\n",
    "            file_words = file_content.split(' ') # must specify ' ' in order not to remove empty strings\n",
    "\n",
    "            prev_entities = None\n",
    "\n",
    "            for word in file_words:\n",
    "                if word.strip() == '':\n",
    "                    # add 1 as those are empty lines but in .ann files they still count as one character\n",
    "                    char_counter += 1\n",
    "                    continue\n",
    "\n",
    "                word_annotations, prev_entities = get_word_annotations(word, annotations, prev_entities, char_counter, len(word))\n",
    "                self._add_stats(word_annotations, output_type)\n",
    "                csv_writer.writerow(word_annotations)\n",
    "\n",
    "                char_counter += len(word) + 1\n",
    "\n",
    "    @property\n",
    "    def stats(self) -> Dict[str, Dict[str, List[str]]]:\n",
    "        return self._stats\n",
    "\n",
    "    def _add_stats(self, word_annotations: Dict[str, str], output_type):\n",
    "        if output_type not in self._stats.keys():\n",
    "            self._stats[output_type] = {}\n",
    "\n",
    "        for col_name, annotations_str in word_annotations.items():\n",
    "            if col_name == 'TOKEN':\n",
    "                continue\n",
    "\n",
    "            if col_name not in self._stats[output_type].keys():\n",
    "                self._stats[output_type][col_name] = []\n",
    "\n",
    "            annotations = annotations_str.split(',')\n",
    "            self._stats[output_type][col_name].extend(annotations)\n",
    "\n",
    "    def close(self):\n",
    "        self._train_file_handler.close()\n",
    "        self._dev_file_handler.close()\n",
    "        self._test_file_handler.close()\n",
    "\n",
    "    def _get_csv_writer(self):\n",
    "        rand = random.random()\n",
    "        if rand <= 0.1:\n",
    "            return self._dev_csv_writer, 'dev'\n",
    "        elif rand <= 0.3:\n",
    "            return self._test_csv_writer, 'test'\n",
    "        else:\n",
    "            return self._train_csv_writer, 'train'\n",
    "\n",
    "    def _prepare_output_file(self, set: str):\n",
    "        output_file = os.path.join(OUTPUT_DIR, f'{set}-nl.tsv')\n",
    "        file_handler = open(output_file, 'w', encoding='utf-8', newline='')\n",
    "        csv_writer = csv.DictWriter(file_handler, fieldnames=self._fieldnames, delimiter='\\t')\n",
    "        csv_writer.writeheader()\n",
    "        return (file_handler, csv_writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_annotation_file(filepath) -> Dict[int, Dict[str, str]]:\n",
    "    annotations_by_pos = {}\n",
    "\n",
    "    # This contains the character positions for any main entity. Example: { 'T1' : [[100, 111], [112, 120]] }\n",
    "    positions_by_main_entity = {}\n",
    "\n",
    "    # Check for lines starting with TN, where N is a numeric value\n",
    "    main_regex = re.compile(f'^[{Constants.MainEntityPrefix}][1-9]+')\n",
    "\n",
    "    # Check for lines starting with AN, where N is a numeric value\n",
    "    sub_regex = re.compile(f'^[{Constants.SubEntityPrefix}][1-9]+')\n",
    "\n",
    "    with open(f'{filepath}.ann', 'r', encoding='utf-8') as file_handle:\n",
    "        file_lines = file_handle.readlines()\n",
    "\n",
    "        for file_line in file_lines:\n",
    "            split_line = file_line.split('\\t')\n",
    "            line_key = split_line[0]\n",
    "\n",
    "            if main_regex.match(line_key): # Main entity type\n",
    "                assert len(split_line) > 1, f'File line is invalid. Not enough tokens were found\\n - Original split line: {split_line}\\n - Filepath: \"{filepath}\"'\n",
    "\n",
    "                # Skip documents that are not valid\n",
    "                if not validate_line(split_line[1]):\n",
    "                    return None\n",
    "\n",
    "                annotation = split_line[1].split()\n",
    "                main_entity_type = annotation[0]\n",
    "\n",
    "                # some positions are doubled, e.g. '100 110; 111 120'\n",
    "                positions = [[int(pos) for pos in x.strip().split()] for x in ' '.join(annotation[1:]).split(';')]\n",
    "                positions_by_main_entity[line_key] = positions\n",
    "\n",
    "                assert len(positions) > 0, f'Positions are invalid.\\n - Original line: {file_line}\\n - Positions: {positions}'\n",
    "                for position_pair in positions:\n",
    "                    assert len(position_pair) == 2, f'Position pair is invalid.\\n - Position pair: {position_pair}'\n",
    "                    for idx in range(position_pair[0], position_pair[1] + 1):\n",
    "                        if idx not in annotations_by_pos.keys():\n",
    "                            annotations_by_pos[idx] = {}\n",
    "                            annotations_by_pos[idx]['main'] = []\n",
    "\n",
    "                        annotations_by_pos[idx]['main'].append(main_entity_type)\n",
    "            elif sub_regex.match(line_key): # Sub entity type\n",
    "                assert len(split_line) > 1, f'{split_line}, {filepath}'\n",
    "                if not validate_line(split_line[1]):\n",
    "                    return None\n",
    "\n",
    "                sub_entity_type, main_entity, sub_entity_value = split_line[1].split()\n",
    "                lowered_sub_entity_type = sub_entity_type.lower()\n",
    "\n",
    "                for position_pair in positions_by_main_entity[main_entity]:\n",
    "                    for idx in range(position_pair[0], position_pair[1] + 1):\n",
    "                        if lowered_sub_entity_type not in annotations_by_pos[idx].keys():\n",
    "                            annotations_by_pos[idx][lowered_sub_entity_type] = []\n",
    "\n",
    "                        annotations_by_pos[idx][sub_entity_type.lower()].append(sub_entity_value)\n",
    "\n",
    "    return annotations_by_pos\n",
    "\n",
    "def get_first_pos_in_annotations(current_position, word_length, annotations, entity_type):\n",
    "    for i in range(current_position, current_position + word_length):\n",
    "        if i in annotations.keys() and (entity_type in annotations[i].keys() or entity_type == 'misc'):\n",
    "            return i\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_misc_comment(current_position, word_length, annotations, entity_type):\n",
    "    annotations_per_main_entity = {}\n",
    "    for i in range(current_position, current_position + word_length):\n",
    "        if i in annotations.keys() and ('main' in annotations[i].keys()):\n",
    "            for entity_tag in annotations[i]['main']:\n",
    "                if entity_tag not in annotations_per_main_entity.keys():\n",
    "                    annotations_per_main_entity[entity_tag] = []\n",
    "\n",
    "                annotations_per_main_entity[entity_tag].append(i)\n",
    "\n",
    "    if len(annotations_per_main_entity) == 0:\n",
    "        return '_'\n",
    "\n",
    "    result = ''\n",
    "    for main_entity_tag, positions in annotations_per_main_entity.items():\n",
    "        if positions[0] > current_position or positions[-1] < (current_position + word_length - 1):\n",
    "            result += f'partial-{main_entity_tag}<{positions[0] - current_position}:{positions[-1] - current_position}>'\n",
    "\n",
    "    if result == '':\n",
    "        return '_'\n",
    "\n",
    "    return result\n",
    "\n",
    "def calculate_entity_tag(annotations, entity_type, prev_entities, current_pos, word_length):\n",
    "    entity = Constants.Empty\n",
    "    valid_pos = get_first_pos_in_annotations(current_pos, word_length, annotations, entity_type)\n",
    "    if valid_pos is not None:\n",
    "        if entity_type == 'misc':\n",
    "            entity = get_misc_comment(current_pos, word_length, annotations, entity_type)\n",
    "        else:\n",
    "            entities = []\n",
    "            for annotation in annotations[valid_pos][entity_type]:\n",
    "                prefix = Constants.Beginning\n",
    "                if annotation in prev_entities[entity_type]:\n",
    "                    prefix = Constants.Inside\n",
    "\n",
    "                entities.append(f'{prefix}{annotation}')\n",
    "\n",
    "            entity = ','.join(entities)\n",
    "            prev_entities[entity_type] = annotations[valid_pos][entity_type]\n",
    "    else:\n",
    "        if entity_type == 'misc':\n",
    "            entity = '_'\n",
    "\n",
    "        prev_entities[entity_type] = Constants.Empty\n",
    "\n",
    "    return entity, prev_entities\n",
    "\n",
    "def get_word_annotations(word, annotations, prev_entities, current_pos, word_length):\n",
    "    result = {\n",
    "        'TOKEN': word,\n",
    "        'MISC': '_'\n",
    "    }\n",
    "\n",
    "    if prev_entities is None:\n",
    "        prev_entities = {x: Constants.Empty for x in entities_to_cols.keys()}\n",
    "\n",
    "    for entity_type, col_name in entities_to_cols.items():\n",
    "        entity, prev_entities = calculate_entity_tag(annotations, entity_type, prev_entities, current_pos, word_length)\n",
    "        result[col_name] = entity\n",
    "\n",
    "    return result, prev_entities\n",
    "\n",
    "def process_files(filepaths):\n",
    "    annotation_writer = AnnotationWriter()\n",
    "    for filepath in tqdm(filepaths, desc='Processing files'):\n",
    "        annotations = process_annotation_file(filepath)\n",
    "        if annotations is None:\n",
    "            continue\n",
    "\n",
    "        annotation_writer.write_annotation(filepath, annotations)\n",
    "\n",
    "    annotation_writer.close()\n",
    "    return annotation_writer.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  69%|██████▉   | 1728/2490 [00:04<00:01, 404.81it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'T2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5c613912af9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-9baa76f5086e>\u001b[0m in \u001b[0;36mprocess_files\u001b[0;34m(filepaths)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mannotation_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnnotationWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Processing files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_annotation_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9baa76f5086e>\u001b[0m in \u001b[0;36mprocess_annotation_file\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mlowered_sub_entity_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_entity_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mposition_pair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositions_by_main_entity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmain_entity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlowered_sub_entity_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotations_by_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'T2'"
     ]
    }
   ],
   "source": [
    "stats = process_files(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(data_stats):\n",
    "    \n",
    "    df_dict = []\n",
    "\n",
    "    for output_type, annotations_per_column in data_stats.items():\n",
    "        for column, annotations in annotations_per_column.items():\n",
    "            if column in ['MISC']:\n",
    "            # if column != 'NE-MAIN':\n",
    "                continue\n",
    "\n",
    "            filtered_annotations = [x.split('-')[-1] for x in annotations if x != 'O']\n",
    "            annotations_counter = Counter(filtered_annotations)\n",
    "\n",
    "            df_dict.extend(\n",
    "                {\n",
    "                    'Output type': output_type,\n",
    "                    'Entity': column,\n",
    "                    'Entity type': x,\n",
    "                    'Count': count\n",
    "                } for x, count in annotations_counter.items()\n",
    "            )\n",
    "\n",
    "    stats_df = pd.DataFrame(df_dict, columns=['Output type', 'Entity', 'Entity type', 'Count'])\n",
    "    for entity in stats_df['Entity'].unique():\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        mask = stats_df['Entity'] == entity\n",
    "        sns.barplot(data=stats_df[mask], x='Output type', y=\"Count\", hue=\"Entity type\", ax=ax)\n",
    "        ax.set_title(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc29f658ddb1b0f0a648f4c47acf5938bc6d1ad3f68ae93354e191176a755a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
