{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import itertools\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('..', 'data_2', 'Keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    doc_annotations = {}\n",
    "    text_lengths = {}\n",
    "\n",
    "    authors_set = set()\n",
    "\n",
    "    for author_folder in os.listdir(DATA_PATH):\n",
    "        authors_set.add(author_folder)\n",
    "        full_path = os.path.join(DATA_PATH, author_folder)\n",
    "        ann_files = list(filter(lambda x: x.endswith('.ann'), os.listdir(full_path)))\n",
    "        for filename in ann_files:\n",
    "            doc_name = os.path.splitext(filename)[0]\n",
    "            if doc_name not in doc_annotations.keys():\n",
    "                doc_annotations[doc_name] = {}\n",
    "\n",
    "            if author_folder in doc_annotations[doc_name].keys():\n",
    "                raise Exception(f'Author \"{author_folder}\" has duplicated annotation for document \"{doc_name}\"')\n",
    "\n",
    "            with open(os.path.join(full_path, filename), 'r') as file_handler:\n",
    "                doc_annotations[doc_name][author_folder] = file_handler.read()\n",
    "\n",
    "            if doc_name not in text_lengths.keys():\n",
    "                with open(os.path.join(full_path, f'{doc_name}.txt'), 'r') as txt_file_handler:\n",
    "                    text_lengths[doc_name] = len(txt_file_handler.read())\n",
    "\n",
    "    return doc_annotations, text_lengths, list(sorted(authors_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_annotations, text_lengths, authors_set = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_annotations = {\n",
    "    k: v for k, v in doc_annotations.items()\n",
    "    if len(v.keys()) > 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation:\n",
    "    def __init__(self, key: str, entity: str, start_pos: int, end_pos: int):\n",
    "        self.key = key\n",
    "        self.entity = entity\n",
    "        self.start_pos = start_pos\n",
    "        self.end_pos = end_pos\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'<{self.key}-{self.entity}-[{self.start_pos}:{self.end_pos}]>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_labels = ['DuplicatePage', 'TranscriptionError_Document']\n",
    "\n",
    "class DocumentAnnotation:\n",
    "    def __init__(self, doc_key: str, annotations_str: str, text_length: int, labels_to_use: List[str]):\n",
    "        self.doc_key = doc_key\n",
    "        self.text_length = text_length\n",
    "        self.is_valid=True\n",
    "\n",
    "        self.annotations = self._parse_annotations(annotations_str, labels_to_use)\n",
    "\n",
    "    def _parse_annotations(self, annotations_str: str, labels_to_use: List[str]):\n",
    "        result = []\n",
    "        annotations = annotations_str.split('\\n')\n",
    "        for annotation in annotations:\n",
    "            annotation_parts = annotation.split('\\t')\n",
    "            if not re.search('([T]{1}[0-9]+)', annotation_parts[0]): continue\n",
    "            if len(annotation_parts) < 2: print(annotation_parts)\n",
    "\n",
    "            split_annotation = annotation_parts[1].split(' ')\n",
    "            label = split_annotation[0]\n",
    "\n",
    "            if label in invalid_labels:\n",
    "                self.is_valid=False\n",
    "                continue\n",
    "\n",
    "            if label not in labels_to_use:\n",
    "                continue\n",
    "\n",
    "            current_annotations_parts = ' '.join(split_annotation[1:]).split(';')\n",
    "\n",
    "            start = int(current_annotations_parts[0].split(' ')[0])\n",
    "            end = int(current_annotations_parts[-1].split(' ')[-1])\n",
    "\n",
    "            result.append(\n",
    "                Annotation(\n",
    "                    key=annotation_parts[0],\n",
    "                    entity=label,\n",
    "                    start_pos=start,\n",
    "                    end_pos=end))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(valid_annotations, text_lengths: Dict[str, int], labels_to_use: List[str]):\n",
    "    parsed_annotations = {\n",
    "        doc_key: {\n",
    "            author: DocumentAnnotation(doc_key, annotations, text_lengths[doc_key], labels_to_use)\n",
    "            for author, annotations in annotations_per_author.items()\n",
    "        }\n",
    "        for doc_key, annotations_per_author in valid_annotations.items()\n",
    "    }\n",
    "\n",
    "    # Remove invalid annotations\n",
    "    parsed_annotations = {\n",
    "        doc_key: {\n",
    "            author: doc_annotation\n",
    "            for author, doc_annotation in annotations_per_author.items()\n",
    "            if doc_annotation.is_valid\n",
    "        }\n",
    "        for doc_key, annotations_per_author in parsed_annotations.items()\n",
    "    }\n",
    "\n",
    "    # Remove documents where we are left with only one annotation\n",
    "    parsed_annotations = {\n",
    "        doc_key: annotations_per_author\n",
    "        for doc_key, annotations_per_author in parsed_annotations.items()\n",
    "        if len(annotations_per_author.keys()) > 1\n",
    "    }\n",
    "\n",
    "    return parsed_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotations_overlap(annotation1: Annotation, annotation2: Annotation, offset_chars: int, match_entity: bool) -> bool:\n",
    "    # if the two annotations do not even overlap with one character, we return false\n",
    "    if (annotation1.start_pos > annotation2.end_pos or\n",
    "        annotation1.end_pos < annotation2.start_pos):\n",
    "        return False\n",
    "\n",
    "    if match_entity and (annotation1.entity != annotation2.entity):\n",
    "        return False\n",
    "\n",
    "    out_of_boundary_chars = abs(annotation1.start_pos - annotation2.start_pos) + abs(annotation1.end_pos - annotation2.end_pos)\n",
    "\n",
    "    result = out_of_boundary_chars <= offset_chars\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mapped_annotations(mapped_annotations: dict):\n",
    "    for ann1, ann2 in mapped_annotations.items():\n",
    "        print(f'{ann1.key} <{ann1.start_pos}-{ann1.end_pos}>  ---', end='')\n",
    "        if ann2 is None:\n",
    "            print('NONE')\n",
    "        else:\n",
    "            print(f'{ann2.key} <{ann2.start_pos}-{ann2.end_pos}>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlapping_annotation(annotation_to_compare: Annotation, annotations: List[Annotation], keys_to_skip: List[str], offset_chars: int, match_entity: bool) -> Annotation:\n",
    "    overlaps = []\n",
    "    for annotation2 in annotations:\n",
    "\n",
    "        if annotation_to_compare.start_pos > annotation2.end_pos:\n",
    "            continue\n",
    "\n",
    "        if annotation2.key in keys_to_skip:\n",
    "            continue\n",
    "\n",
    "        if annotations_overlap(annotation_to_compare, annotation2, offset_chars, match_entity):\n",
    "            overlaps.append(annotation2)\n",
    "\n",
    "    if len(overlaps) == 0:\n",
    "        return None\n",
    "\n",
    "    for overlap in overlaps:\n",
    "        if overlap.entity == annotation_to_compare.entity:\n",
    "            return overlap\n",
    "\n",
    "    return overlaps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entity_overlap(doc_annotation1: DocumentAnnotation, doc_annotation2: DocumentAnnotation, offset_chars: int, debug: bool = False):\n",
    "    assert (doc_annotation1.doc_key == doc_annotation2.doc_key)\n",
    "\n",
    "    mapped_annotations = {}\n",
    "    used_counter_annotations = set()\n",
    "    empty_positions = [1 for _ in range(0, doc_annotation1.text_length)]\n",
    "    for annotation in doc_annotation1.annotations:\n",
    "        for i in range(annotation.start_pos, annotation.end_pos):\n",
    "            empty_positions[i] = 0\n",
    "\n",
    "    # Perform iteration using strict overlap matching\n",
    "    for annotation in doc_annotation1.annotations:\n",
    "        overlapping_annotation = get_overlapping_annotation(annotation, doc_annotation2.annotations, used_counter_annotations, offset_chars, match_entity=True)\n",
    "        if overlapping_annotation is not None:\n",
    "            used_counter_annotations.add(overlapping_annotation.key)\n",
    "            mapped_annotations[annotation] = overlapping_annotation\n",
    "\n",
    "    # Perform iteration using loose overlap matching\n",
    "    for annotation in doc_annotation1.annotations:\n",
    "        if annotation in mapped_annotations.keys():\n",
    "            continue\n",
    "\n",
    "        overlapping_annotation = get_overlapping_annotation(annotation, doc_annotation2.annotations, used_counter_annotations, offset_chars, match_entity=False)\n",
    "        if overlapping_annotation is not None:\n",
    "            used_counter_annotations.add(overlapping_annotation.key)\n",
    "\n",
    "        mapped_annotations[annotation] = overlapping_annotation\n",
    "\n",
    "    if debug:\n",
    "        print_mapped_annotations(mapped_annotations)\n",
    "    annotation_maps = [ x.entity for x in mapped_annotations.keys() ]\n",
    "    counter_annotations = [ x.entity if x is not None else 'O' for x in mapped_annotations.values() ]\n",
    "\n",
    "    for annotation2 in doc_annotation2.annotations:\n",
    "        for i in range(annotation2.start_pos, annotation2.end_pos):\n",
    "            empty_positions[i] = 0\n",
    "\n",
    "        if annotation2.key in used_counter_annotations:\n",
    "            continue\n",
    "\n",
    "        annotation_maps.append('O')\n",
    "        counter_annotations.append(annotation2.entity)\n",
    "\n",
    "    free_positions = sum(empty_positions)\n",
    "    for _ in range(free_positions):\n",
    "        annotation_maps.append('O')\n",
    "        counter_annotations.append('O')\n",
    "\n",
    "    if annotation_maps == counter_annotations:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = cohen_kappa_score(annotation_maps, counter_annotations)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_matrix(parsed_annotations, offset_chars: int, authors_set: set):\n",
    "    comparisons = {\n",
    "        author_1 : {\n",
    "            author_2: []\n",
    "            for author_2 in authors_set\n",
    "        } for author_1 in authors_set\n",
    "    }\n",
    "\n",
    "    for _, annotations in parsed_annotations.items():\n",
    "        for author_1, author_2 in itertools.product(authors_set, authors_set):\n",
    "            if author_1 in annotations.keys() and author_2 in annotations.keys():\n",
    "                debug=False\n",
    "                kappa_score = calculate_entity_overlap(annotations[author_1], annotations[author_2], offset_chars, debug=debug)\n",
    "\n",
    "                comparisons[author_1][author_2].append(kappa_score)\n",
    "\n",
    "        for author in authors_set:\n",
    "            comparisons[author][author] = [1]\n",
    "\n",
    "    return comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison_matrix(comparisons, authors_set: List[str], offset_chars: int, labels_to_use: List[str]):\n",
    "    print (f'Results, offset characters: {offset_chars}, used labels: <{\",\".join(labels_to_use)}>')\n",
    "    print('\\t', end='')\n",
    "    print('\\t'.join(authors_set))\n",
    "    for author_1 in authors_set:\n",
    "        print(author_1, end='\\t')\n",
    "        for author_2 in authors_set:\n",
    "            print(round(np.mean(comparisons[author_1][author_2]), 2), end='\\t')\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison(valid_annotations, text_lengths: Dict[str, int], authors_set: List[str], offset_chars: int, labels_to_use: List[str]):\n",
    "    parsed_annotations = parse_annotations(valid_annotations, text_lengths, labels_to_use)\n",
    "    comparison_matrix = create_comparison_matrix(parsed_annotations, offset_chars, authors_set)\n",
    "    print_comparison_matrix(comparison_matrix, authors_set, offset_chars, labels_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results, offset characters: 0, used labels: <Person,Place,Organization>\n",
      "\tBert\tEmma\tJonas\tRoos\tSilja\tYolien\n",
      "Bert\t1.0\t0.43\t0.42\t0.44\t0.5\t0.45\t\n",
      "Emma\t0.43\t1.0\t0.47\t0.63\t0.35\t0.47\t\n",
      "Jonas\t0.42\t0.47\t1.0\t0.48\t0.47\t0.38\t\n",
      "Roos\t0.44\t0.63\t0.48\t1.0\t0.55\t0.47\t\n",
      "Silja\t0.5\t0.35\t0.47\t0.55\t1.0\t0.48\t\n",
      "Yolien\t0.45\t0.47\t0.38\t0.47\t0.48\t1.0\t\n"
     ]
    }
   ],
   "source": [
    "run_comparison(valid_annotations, text_lengths, authors_set, offset_chars=0, labels_to_use=['Person', 'Place', 'Organization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results, offset characters: 0, used labels: <Person,Place,Organization,ProperName,Noteworthy>\n",
      "\tBert\tEmma\tJonas\tRoos\tSilja\tYolien\n",
      "Bert\t1.0\t0.53\t0.51\t0.46\t0.64\t0.64\t\n",
      "Emma\t0.53\t1.0\t0.5\t0.69\t0.5\t0.52\t\n",
      "Jonas\t0.51\t0.5\t1.0\t0.55\t0.49\t0.43\t\n",
      "Roos\t0.46\t0.69\t0.55\t1.0\t0.62\t0.56\t\n",
      "Silja\t0.64\t0.5\t0.49\t0.62\t1.0\t0.48\t\n",
      "Yolien\t0.64\t0.52\t0.43\t0.56\t0.48\t1.0\t\n"
     ]
    }
   ],
   "source": [
    "run_comparison(valid_annotations, text_lengths, authors_set, offset_chars=0, labels_to_use=['Person', 'Place', 'Organization', 'ProperName', 'Noteworthy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results, offset characters: 50, used labels: <Person,Place,Organization>\n",
      "\tBert\tEmma\tJonas\tRoos\tSilja\tYolien\n",
      "Bert\t1.0\t0.77\t0.74\t0.65\t0.81\t0.89\t\n",
      "Emma\t0.77\t1.0\t0.85\t0.89\t0.85\t0.88\t\n",
      "Jonas\t0.74\t0.85\t1.0\t0.78\t0.81\t0.81\t\n",
      "Roos\t0.65\t0.89\t0.78\t1.0\t0.85\t0.85\t\n",
      "Silja\t0.81\t0.85\t0.81\t0.85\t1.0\t0.79\t\n",
      "Yolien\t0.89\t0.88\t0.81\t0.85\t0.79\t1.0\t\n"
     ]
    }
   ],
   "source": [
    "run_comparison(valid_annotations, text_lengths, authors_set, offset_chars=50, labels_to_use=['Person', 'Place', 'Organization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results, offset characters: 500, used labels: <Person,Place,Organization>\n",
      "\tBert\tEmma\tJonas\tRoos\tSilja\tYolien\n",
      "Bert\t1.0\t0.78\t0.74\t0.65\t0.81\t0.89\t\n",
      "Emma\t0.78\t1.0\t0.87\t0.9\t0.85\t0.88\t\n",
      "Jonas\t0.74\t0.87\t1.0\t0.82\t0.83\t0.86\t\n",
      "Roos\t0.65\t0.9\t0.82\t1.0\t0.87\t0.85\t\n",
      "Silja\t0.81\t0.85\t0.83\t0.87\t1.0\t0.8\t\n",
      "Yolien\t0.89\t0.88\t0.86\t0.85\t0.8\t1.0\t\n"
     ]
    }
   ],
   "source": [
    "run_comparison(valid_annotations, text_lengths, authors_set, offset_chars=500, labels_to_use=['Person', 'Place', 'Organization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results, offset characters: 500, used labels: <Person,Place,Organization,ProperName,Noteworthy>\n",
      "\tBert\tEmma\tJonas\tRoos\tSilja\tYolien\n",
      "Bert\t1.0\t0.77\t0.71\t0.66\t0.83\t0.91\t\n",
      "Emma\t0.77\t1.0\t0.77\t0.89\t0.88\t0.88\t\n",
      "Jonas\t0.71\t0.77\t1.0\t0.76\t0.77\t0.78\t\n",
      "Roos\t0.66\t0.89\t0.76\t1.0\t0.84\t0.84\t\n",
      "Silja\t0.83\t0.88\t0.77\t0.84\t1.0\t0.8\t\n",
      "Yolien\t0.91\t0.88\t0.78\t0.84\t0.8\t1.0\t\n"
     ]
    }
   ],
   "source": [
    "run_comparison(valid_annotations, text_lengths, authors_set, offset_chars=500, labels_to_use=['Person', 'Place', 'Organization', 'ProperName', 'Noteworthy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b75f3199d40dc5b32c50d7e73d50b0653ef4e42fc29a92d9ddb9ff9d4b03964"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ocr': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
