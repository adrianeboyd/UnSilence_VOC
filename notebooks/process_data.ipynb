{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from typing import Dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "OUTPUT_DIR = os.path.join('..', '..', 'processed')\n",
    "DATA_PATH = os.path.join('..', '..', '2021-national-archives-data-annotation-project', 'data')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def should_use_file(filepath) -> bool:\n",
    "    # if file path is wrong for some reason, do not use it\n",
    "    if not os.path.exists(filepath):\n",
    "        return False\n",
    "\n",
    "    file_stats = os.stat(filepath)\n",
    "\n",
    "    # if file is empty, do not use it\n",
    "    if file_stats.st_size == 0:\n",
    "        return False\n",
    "\n",
    "    return True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def get_filepaths(data_path, folders_to_ignore = []):\n",
    "    result = []\n",
    "    for folder_name in os.listdir(data_path):\n",
    "        if folder_name in folders_to_ignore:\n",
    "            continue\n",
    "\n",
    "        folder_path = os.path.join(data_path, folder_name)\n",
    "\n",
    "        if not os.path.isdir(folder_path): # it's a file\n",
    "            if folder_path.endswith('.ann') and should_use_file(folder_path): # only work with .ann files\n",
    "                result.append(os.path.splitext(folder_path)[0])\n",
    "\n",
    "            continue\n",
    "\n",
    "        sub_paths = get_filepaths(folder_path)\n",
    "        if len(sub_paths) > 0:\n",
    "            result.extend(sub_paths)\n",
    "\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "filepaths = get_filepaths(DATA_PATH, folders_to_ignore=['6847', 'Charles'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "len(filepaths)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2889"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def validate_line(line_text):\n",
    "    '''\n",
    "        Validate if a line is not marked as 'transcription error' or as a duplicated one.\n",
    "        If any of those is true, this whole document must be skipped\n",
    "    '''\n",
    "    invalid = line_text.startswith('TranscriptionError') or line_text.startswith('DuplicatePage')\n",
    "    return not invalid"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class Constants():\n",
    "    Empty = 'O'\n",
    "    Beginning = 'B-'\n",
    "    Inside = 'I-'\n",
    "\n",
    "    MainEntityPrefix = 'T'\n",
    "    SubEntityPrefix = 'A'\n",
    "\n",
    "entities_to_cols = {\n",
    "    'main': 'NE-MAIN',\n",
    "    'gender': 'NE-PER-GENDER',\n",
    "    'legalstatus': 'NE-PER-LEGAL-STATUS',\n",
    "    'role': 'NE-PER-ROLE',\n",
    "    'misc': 'MISC',\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def prepare_output_file():\n",
    "    output_file = os.path.join(OUTPUT_DIR, 'train-nl.tsv')\n",
    "    file_handler = open(output_file, 'w', encoding='utf-8', newline='')\n",
    "    csv_writer = csv.DictWriter(file_handler, fieldnames=['TOKEN', 'NE-MAIN', 'NE-PER-GENDER', 'NE-PER-LEGAL-STATUS', 'NE-PER-ROLE', 'MISC'], delimiter='\\t')\n",
    "    csv_writer.writeheader()\n",
    "    return (file_handler, csv_writer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_annotator_from_filepath(filepath:str):\n",
    "    normalized_path = os.path.normpath(filepath)\n",
    "    split_path = normalized_path.split(os.sep)\n",
    "    main_folder_index = split_path.index('2021-national-archives-data-annotation-project')\n",
    "    annotator = split_path[main_folder_index + 2]\n",
    "\n",
    "    return annotator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def process_annotation_file(filepath) -> Dict[int, Dict[str, str]]:\n",
    "    # \n",
    "    annotations_by_pos = {}\n",
    "\n",
    "    # This contains the character positions for any main entity. Example: { 'T1' : [[100, 111], [112, 120]] }\n",
    "    positions_by_main_entity = {}\n",
    "\n",
    "    # Check for lines starting with TN, where N is a numeric value\n",
    "    main_regex = re.compile(f'^[{Constants.MainEntityPrefix}][1-9]+')\n",
    "\n",
    "    # Check for lines starting with AN, where N is a numeric value\n",
    "    sub_regex = re.compile(f'^[{Constants.SubEntityPrefix}][1-9]+')\n",
    "\n",
    "    with open(f'{filepath}.ann', 'r', encoding='utf-8') as file_handle:\n",
    "        file_lines = file_handle.readlines()\n",
    "\n",
    "        for file_line in file_lines:\n",
    "            split_line = file_line.split('\\t')\n",
    "            line_key = split_line[0]\n",
    "\n",
    "            if main_regex.match(line_key): # Main entity type\n",
    "                assert len(split_line) > 1, f'File line is invalid. Not enough tokens were found\\n - Original split line: {split_line}\\n - Filepath: \"{filepath}\"'\n",
    "\n",
    "                # Skip documents that are not valid\n",
    "                if not validate_line(split_line[1]):\n",
    "                    return None\n",
    "\n",
    "                annotation = split_line[1].split()\n",
    "                main_entity_type = annotation[0]\n",
    "\n",
    "                # some positions are doubled, e.g. '100 110; 111 120'\n",
    "                positions = [[int(pos) for pos in x.strip().split()] for x in ' '.join(annotation[1:]).split(';')]\n",
    "                positions_by_main_entity[line_key] = positions\n",
    "\n",
    "                assert len(positions) > 0, f'Positions are invalid.\\n - Original line: {file_line}\\n - Positions: {positions}'\n",
    "                for position_pair in positions:\n",
    "                    assert len(position_pair) == 2, f'Position pair is invalid.\\n - Position pair: {position_pair}'\n",
    "                    for idx in range(position_pair[0], position_pair[1] + 1):\n",
    "                        if idx not in annotations_by_pos.keys():\n",
    "                            annotations_by_pos[idx] = {}\n",
    "                            annotations_by_pos[idx]['main'] = []\n",
    "\n",
    "                        annotations_by_pos[idx]['main'].append(main_entity_type)\n",
    "            elif sub_regex.match(line_key): # Sub entity type\n",
    "                assert len(split_line) > 1, f'{split_line}, {filepath}'\n",
    "                if not validate_line(split_line[1]):\n",
    "                    return None\n",
    "\n",
    "                sub_entity_type, main_entity, sub_entity_value = split_line[1].split()\n",
    "                lowered_sub_entity_type = sub_entity_type.lower()\n",
    "\n",
    "                for position_pair in positions_by_main_entity[main_entity]:\n",
    "                    for idx in range(position_pair[0], position_pair[1] + 1):\n",
    "                        if lowered_sub_entity_type not in annotations_by_pos[idx].keys():\n",
    "                            annotations_by_pos[idx][lowered_sub_entity_type] = []\n",
    "\n",
    "                        annotations_by_pos[idx][sub_entity_type.lower()].append(sub_entity_value)\n",
    "\n",
    "    return annotations_by_pos\n",
    "\n",
    "def get_first_pos_in_annotations(current_position, word_length, annotations, entity_type):\n",
    "    result = []\n",
    "    for i in range(current_position, current_position + word_length):\n",
    "        if i in annotations.keys() and (entity_type in annotations[i].keys() or entity_type == 'misc'):\n",
    "            result.append(i)\n",
    "\n",
    "    if len(result) == 0:\n",
    "        return None, None\n",
    "\n",
    "    transformed_result = [x - current_position for x in result]\n",
    "    return result[0], transformed_result\n",
    "\n",
    "def calculate_entity_tag(annotations, entity_type, prev_entities, current_pos, word_length):\n",
    "    entity = Constants.Empty\n",
    "    valid_pos, annotated_positions = get_first_pos_in_annotations(current_pos, word_length, annotations, entity_type)\n",
    "    if valid_pos is not None:\n",
    "        if entity_type == 'misc':\n",
    "            entity = '_'\n",
    "            if len(annotated_positions) > 1 and len(annotated_positions) < word_length:\n",
    "                # entity = f'partial-{\",\".join([str(x-current_pos) for x in annotated_positions])};wl-{word_length}'\n",
    "                entity = f'partial<{annotated_positions[0]}:{annotated_positions[-1]}>'\n",
    "        else:\n",
    "            entities = []\n",
    "            for annotation in annotations[valid_pos][entity_type]:\n",
    "                prefix = Constants.Beginning\n",
    "                if annotation in prev_entities[entity_type]:\n",
    "                    prefix = Constants.Inside\n",
    "\n",
    "                entities.append(f'{prefix}{annotation}')\n",
    "\n",
    "            entity = ','.join(entities)\n",
    "            prev_entities[entity_type] = annotations[valid_pos][entity_type]\n",
    "    else:\n",
    "        if entity_type == 'misc':\n",
    "            entity = '_'\n",
    "\n",
    "        prev_entities[entity_type] = Constants.Empty\n",
    "\n",
    "    return entity, prev_entities\n",
    "\n",
    "def get_word_annotations(word, annotations, prev_entities, current_pos, word_length):\n",
    "    result = {\n",
    "        'TOKEN': word,\n",
    "        'MISC': '_'\n",
    "    }\n",
    "\n",
    "    if prev_entities is None:\n",
    "        prev_entities = {x: Constants.Empty for x in entities_to_cols.keys()}\n",
    "\n",
    "    for entity_type, col_name in entities_to_cols.items():\n",
    "        entity, prev_entities = calculate_entity_tag(annotations, entity_type, prev_entities, current_pos, word_length)\n",
    "        result[col_name] = entity\n",
    "\n",
    "    return result, prev_entities\n",
    "\n",
    "def process_txt_file(filepath, annotations, csv_writer: csv.DictWriter):\n",
    "    char_counter = 1\n",
    "\n",
    "    csv_writer.writerow({})\n",
    "    csv_writer.writerow({'TOKEN': '# language = nl'})\n",
    "    csv_writer.writerow({'TOKEN': f'# document_path = {filepath}.txt'})\n",
    "    annotator = get_annotator_from_filepath(filepath)\n",
    "    csv_writer.writerow({'TOKEN': f'# annotator = {annotator}'})\n",
    "\n",
    "    with open(f'{filepath}.txt', 'r', encoding='utf-8') as file_handle:\n",
    "        file_content = file_handle.read().replace('\\n', ' ')\n",
    "        file_words = file_content.split(' ') # must specify ' ' in order not to remove empty strings\n",
    "\n",
    "        prev_entities = None\n",
    "\n",
    "        for word in file_words:\n",
    "            word_annotations, prev_entities = get_word_annotations(word, annotations, prev_entities, char_counter, len(word))\n",
    "            csv_writer.writerow(word_annotations)\n",
    "\n",
    "            char_counter += len(word) + 1\n",
    "\n",
    "\n",
    "def process_files(filepaths):\n",
    "    file_handler, csv_writer = prepare_output_file()\n",
    "\n",
    "    for filepath in tqdm(filepaths, desc='Processing files'):\n",
    "        annotations = process_annotation_file(filepath)\n",
    "        if annotations is None:\n",
    "            continue\n",
    "\n",
    "        process_txt_file(filepath, annotations, csv_writer)\n",
    "\n",
    "    file_handler.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "process_files(filepaths)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Processing files: 100%|██████████| 2889/2889 [00:07<00:00, 366.11it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "d660214d6c40f1e1052e39c34b9aef9223105e202e3dbfbec4fa8fd97ab6054c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}