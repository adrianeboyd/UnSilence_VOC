{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "import csv\r\n",
    "from tqdm import tqdm\r\n",
    "import re\r\n",
    "\r\n",
    "from typing import Dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "OUTPUT_DIR = os.path.join('..', 'processed_data')\r\n",
    "DATA_PATH = os.path.join('..', '..', '2021-national-archives-data-annotation-project', 'data')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def should_use_file(filepath) -> bool:\r\n",
    "    # if file path is wrong for some reason, do not use it\r\n",
    "    if not os.path.exists(filepath):\r\n",
    "        return False\r\n",
    "\r\n",
    "    file_stats = os.stat(filepath)\r\n",
    "\r\n",
    "    # if file is empty, do not use it\r\n",
    "    if file_stats.st_size == 0:\r\n",
    "        return False\r\n",
    "\r\n",
    "    return True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_filepaths(data_path, folders_to_ignore = []):\r\n",
    "    result = []\r\n",
    "    for folder_name in os.listdir(data_path):\r\n",
    "        if folder_name in folders_to_ignore:\r\n",
    "            continue\r\n",
    "\r\n",
    "        folder_path = os.path.join(data_path, folder_name)\r\n",
    "\r\n",
    "        if not os.path.isdir(folder_path): # it's a file\r\n",
    "            if folder_path.endswith('.ann') and should_use_file(folder_path): # only work with .ann files\r\n",
    "                result.append(os.path.splitext(folder_path)[0])\r\n",
    "\r\n",
    "            continue\r\n",
    "\r\n",
    "        sub_paths = get_filepaths(folder_path)\r\n",
    "        if len(sub_paths) > 0:\r\n",
    "            result.extend(sub_paths)\r\n",
    "\r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "filepaths = get_filepaths(DATA_PATH, folders_to_ignore=['6847', 'Charles'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "len(filepaths)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2889"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def validate_line(line_text):\r\n",
    "    '''\r\n",
    "        Validate if a line is not marked as 'transcription error' or as a duplicated one.\r\n",
    "        If any of those is true, this whole document must be skipped\r\n",
    "    '''\r\n",
    "    invalid = line_text.startswith('TranscriptionError') or line_text.startswith('DuplicatePage')\r\n",
    "    return not invalid"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class Constants():\r\n",
    "    Empty = 'O'\r\n",
    "    Beginning = 'B-'\r\n",
    "    Inside = 'I-'\r\n",
    "\r\n",
    "    MainEntityPrefix = 'T'\r\n",
    "    SubEntityPrefix = 'A'\r\n",
    "\r\n",
    "entities_to_cols = {\r\n",
    "    'main': 'NE-MAIN',\r\n",
    "    'gender': 'NE-PER-GENDER',\r\n",
    "    'legalstatus': 'NE-PER-LEGAL-STATUS',\r\n",
    "    'role': 'NE-PER-ROLE',\r\n",
    "    'misc': 'MISC',\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def prepare_output_file():\r\n",
    "    output_file = os.path.join(OUTPUT_DIR, 'train-nl.tsv')\r\n",
    "    file_handler = open(output_file, 'w', encoding='utf-8', newline='')\r\n",
    "    csv_writer = csv.DictWriter(file_handler, fieldnames=['TOKEN', 'NE-MAIN', 'NE-PER-GENDER', 'NE-PER-LEGAL-STATUS', 'NE-PER-ROLE', 'MISC'], delimiter='\\t')\r\n",
    "    csv_writer.writeheader()\r\n",
    "    return (file_handler, csv_writer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def get_annotator_from_filepath(filepath:str):\r\n",
    "    normalized_path = os.path.normpath(filepath)\r\n",
    "    split_path = normalized_path.split(os.sep)\r\n",
    "    main_folder_index = split_path.index('2021-national-archives-data-annotation-project')\r\n",
    "    annotator = split_path[main_folder_index + 2]\r\n",
    "\r\n",
    "    return annotator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "def process_annotation_file(filepath) -> Dict[int, Dict[str, str]]:\r\n",
    "    # \r\n",
    "    annotations_by_pos = {}\r\n",
    "\r\n",
    "    # This contains the character positions for any main entity. Example: { 'T1' : [[100, 111], [112, 120]] }\r\n",
    "    positions_by_main_entity = {}\r\n",
    "\r\n",
    "    # Check for lines starting with TN, where N is a numeric value\r\n",
    "    main_regex = re.compile(f'^[{Constants.MainEntityPrefix}][1-9]+')\r\n",
    "\r\n",
    "    # Check for lines starting with AN, where N is a numeric value\r\n",
    "    sub_regex = re.compile(f'^[{Constants.SubEntityPrefix}][1-9]+')\r\n",
    "\r\n",
    "    with open(f'{filepath}.ann', 'r', encoding='utf-8') as file_handle:\r\n",
    "        file_lines = file_handle.readlines()\r\n",
    "\r\n",
    "        for file_line in file_lines:\r\n",
    "            split_line = file_line.split('\\t')\r\n",
    "            line_key = split_line[0]\r\n",
    "\r\n",
    "            if main_regex.match(line_key): # Main entity type\r\n",
    "                assert len(split_line) > 1, f'File line is invalid. Not enough tokens were found\\n - Original split line: {split_line}\\n - Filepath: \"{filepath}\"'\r\n",
    "\r\n",
    "                # Skip documents that are not valid\r\n",
    "                if not validate_line(split_line[1]):\r\n",
    "                    return None\r\n",
    "\r\n",
    "                annotation = split_line[1].split()\r\n",
    "                main_entity_type = annotation[0]\r\n",
    "\r\n",
    "                # some positions are doubled, e.g. '100 110; 111 120'\r\n",
    "                positions = [[int(pos) for pos in x.strip().split()] for x in ' '.join(annotation[1:]).split(';')]\r\n",
    "                positions_by_main_entity[line_key] = positions\r\n",
    "\r\n",
    "                assert len(positions) > 0, f'Positions are invalid.\\n - Original line: {file_line}\\n - Positions: {positions}'\r\n",
    "                for position_pair in positions:\r\n",
    "                    assert len(position_pair) == 2, f'Position pair is invalid.\\n - Position pair: {position_pair}'\r\n",
    "                    for idx in range(position_pair[0], position_pair[1] + 1):\r\n",
    "                        if idx not in annotations_by_pos.keys():\r\n",
    "                            annotations_by_pos[idx] = {}\r\n",
    "                            annotations_by_pos[idx]['main'] = []\r\n",
    "\r\n",
    "                        annotations_by_pos[idx]['main'].append(main_entity_type)\r\n",
    "            elif sub_regex.match(line_key): # Sub entity type\r\n",
    "                assert len(split_line) > 1, f'{split_line}, {filepath}'\r\n",
    "                if not validate_line(split_line[1]):\r\n",
    "                    return None\r\n",
    "\r\n",
    "                sub_entity_type, main_entity, sub_entity_value = split_line[1].split()\r\n",
    "                lowered_sub_entity_type = sub_entity_type.lower()\r\n",
    "\r\n",
    "                for position_pair in positions_by_main_entity[main_entity]:\r\n",
    "                    for idx in range(position_pair[0], position_pair[1] + 1):\r\n",
    "                        if lowered_sub_entity_type not in annotations_by_pos[idx].keys():\r\n",
    "                            annotations_by_pos[idx][lowered_sub_entity_type] = []\r\n",
    "\r\n",
    "                        annotations_by_pos[idx][sub_entity_type.lower()].append(sub_entity_value)\r\n",
    "\r\n",
    "    return annotations_by_pos\r\n",
    "\r\n",
    "def get_first_pos_in_annotations(current_position, word_length, annotations, entity_type):\r\n",
    "    for i in range(current_position, current_position + word_length):\r\n",
    "        if i in annotations.keys() and (entity_type in annotations[i].keys() or entity_type == 'misc'):\r\n",
    "            return i\r\n",
    "\r\n",
    "    return None\r\n",
    "\r\n",
    "def get_misc_comment(current_position, word_length, annotations, entity_type):\r\n",
    "    annotations_per_main_entity = {}\r\n",
    "    for i in range(current_position, current_position + word_length):\r\n",
    "        if i in annotations.keys() and ('main' in annotations[i].keys()):\r\n",
    "            for entity_tag in annotations[i]['main']:\r\n",
    "                if entity_tag not in annotations_per_main_entity.keys():\r\n",
    "                    annotations_per_main_entity[entity_tag] = []\r\n",
    "\r\n",
    "                annotations_per_main_entity[entity_tag].append(i)\r\n",
    "\r\n",
    "    if len(annotations_per_main_entity) == 0:\r\n",
    "        return '_'\r\n",
    "\r\n",
    "    result = ''\r\n",
    "    for main_entity_tag, positions in annotations_per_main_entity.items():\r\n",
    "        if positions[0] > current_position or positions[-1] < (current_position + word_length - 1):\r\n",
    "            result += f'partial-{main_entity_tag}<{positions[0] - current_position}:{positions[-1] - current_position}>'\r\n",
    "\r\n",
    "    if result == '':\r\n",
    "        return '_'\r\n",
    "\r\n",
    "    return result\r\n",
    "\r\n",
    "def calculate_entity_tag(annotations, entity_type, prev_entities, current_pos, word_length):\r\n",
    "    entity = Constants.Empty\r\n",
    "    valid_pos = get_first_pos_in_annotations(current_pos, word_length, annotations, entity_type)\r\n",
    "    if valid_pos is not None:\r\n",
    "        if entity_type == 'misc':\r\n",
    "            entity = get_misc_comment(current_pos, word_length, annotations, entity_type)\r\n",
    "        else:\r\n",
    "            entities = []\r\n",
    "            for annotation in annotations[valid_pos][entity_type]:\r\n",
    "                prefix = Constants.Beginning\r\n",
    "                if annotation in prev_entities[entity_type]:\r\n",
    "                    prefix = Constants.Inside\r\n",
    "\r\n",
    "                entities.append(f'{prefix}{annotation}')\r\n",
    "\r\n",
    "            entity = ','.join(entities)\r\n",
    "            prev_entities[entity_type] = annotations[valid_pos][entity_type]\r\n",
    "    else:\r\n",
    "        if entity_type == 'misc':\r\n",
    "            entity = '_'\r\n",
    "\r\n",
    "        prev_entities[entity_type] = Constants.Empty\r\n",
    "\r\n",
    "    return entity, prev_entities\r\n",
    "\r\n",
    "def get_word_annotations(word, annotations, prev_entities, current_pos, word_length):\r\n",
    "    result = {\r\n",
    "        'TOKEN': word,\r\n",
    "        'MISC': '_'\r\n",
    "    }\r\n",
    "\r\n",
    "    if prev_entities is None:\r\n",
    "        prev_entities = {x: Constants.Empty for x in entities_to_cols.keys()}\r\n",
    "\r\n",
    "    for entity_type, col_name in entities_to_cols.items():\r\n",
    "        entity, prev_entities = calculate_entity_tag(annotations, entity_type, prev_entities, current_pos, word_length)\r\n",
    "        result[col_name] = entity\r\n",
    "\r\n",
    "    return result, prev_entities\r\n",
    "\r\n",
    "def process_txt_file(filepath, annotations, csv_writer: csv.DictWriter):\r\n",
    "    char_counter = 1\r\n",
    "\r\n",
    "    csv_writer.writerow({})\r\n",
    "    csv_writer.writerow({'TOKEN': '# language = nl'})\r\n",
    "    csv_writer.writerow({'TOKEN': f'# document_path = {filepath}.txt'})\r\n",
    "    annotator = get_annotator_from_filepath(filepath)\r\n",
    "    csv_writer.writerow({'TOKEN': f'# annotator = {annotator}'})\r\n",
    "\r\n",
    "    with open(f'{filepath}.txt', 'r', encoding='utf-8') as file_handle:\r\n",
    "        file_content = file_handle.read().replace('\\n', ' ')\r\n",
    "        file_words = file_content.split(' ') # must specify ' ' in order not to remove empty strings\r\n",
    "\r\n",
    "        prev_entities = None\r\n",
    "\r\n",
    "        for word in file_words:\r\n",
    "            if word.strip() == '':\r\n",
    "                # add 1 as those are empty lines but in .ann files they still count as one character\r\n",
    "                char_counter += 1\r\n",
    "                continue\r\n",
    "\r\n",
    "            word_annotations, prev_entities = get_word_annotations(word, annotations, prev_entities, char_counter, len(word))\r\n",
    "            csv_writer.writerow(word_annotations)\r\n",
    "\r\n",
    "            char_counter += len(word) + 1\r\n",
    "\r\n",
    "\r\n",
    "def process_files(filepaths):\r\n",
    "    file_handler, csv_writer = prepare_output_file()\r\n",
    "    for filepath in tqdm(filepaths, desc='Processing files'):\r\n",
    "        annotations = process_annotation_file(filepath)\r\n",
    "        if annotations is None:\r\n",
    "            continue\r\n",
    "\r\n",
    "        process_txt_file(filepath, annotations, csv_writer)\r\n",
    "\r\n",
    "    file_handler.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "process_files(filepaths)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Processing files: 100%|██████████| 2889/2889 [00:05<00:00, 517.78it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('ocr-uva': conda)"
  },
  "interpreter": {
   "hash": "cc29f658ddb1b0f0a648f4c47acf5938bc6d1ad3f68ae93354e191176a755a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}